{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Language Learning, Lab1\n",
    "\n",
    "## Adriaan de Vries (10795227), Verna Dankers (10761225)\n",
    "\n",
    "Hier komt een verhaaltje over de eerste opdracht.\n",
    "\n",
    "Before being able to run this code, please import the following libraries and set the following paths to the datasets. Afterwards, the code should run without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from tabulate import tabulate\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to datasets\n",
    "bow2_filename = \"data/bow2.words\"\n",
    "bow5_filename = \"data/bow5.words\"\n",
    "deps_filename = \"data/deps.words\"\n",
    "simlex_filename = \"data/SimLex-999.txt\"\n",
    "men_filename = \"data/men/MEN_dataset_natural_form_full\"\n",
    "analogy_filename = \"data/questions-words.txt\"\n",
    "common_words_filename = \"data/common_words.words\"\n",
    "\n",
    "glove2word2vec(bow2_filename, bow2_filename.split(\".\")[0] + \".txt\")\n",
    "glove2word2vec(bow5_filename, bow5_filename.split(\".\")[0] + \".txt\")\n",
    "glove2word2vec(deps_filename, deps_filename.split(\".\")[0] + \".txt\")\n",
    "bow2 = KeyedVectors.load_word2vec_format(bow2_filename.split(\".\")[0] + \".txt\", binary=False)\n",
    "bow2.init_sims(replace=True)\n",
    "bow5 = KeyedVectors.load_word2vec_format(bow5_filename.split(\".\")[0] + \".txt\", binary=False)\n",
    "bow5.init_sims(replace=True)\n",
    "deps = KeyedVectors.load_word2vec_format(deps_filename.split(\".\")[0] + \".txt\", binary=False)\n",
    "deps.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collect and examine the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deps.most_similar(positive=['reddish'])[0])\n",
    "print(bow5.most_similar(positive=['cop'])[0])\n",
    "print(bow2.most_similar(positive=['tissue'])[0])\n",
    "\n",
    "print(deps.most_similar(positive=['sudoku'])[0])\n",
    "print(bow5.most_similar(positive=['sudoku'])[0])\n",
    "print(bow2.most_similar(positive=['sudoku'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Collect the SimLex and MEN data to evaluate the quality of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_men(men, embeddings):\n",
    "    scores = [[], []]\n",
    "    for pair in men:\n",
    "        try:\n",
    "            scores[0].append(embeddings.similarity(pair[0], pair[1]))\n",
    "            scores[1].append(men[pair])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return (scores[0], scores[1])\n",
    "\n",
    "def score_simlex(simlex, name, embeddings):\n",
    "    scores = [[], []]\n",
    "    for pair in simlex:\n",
    "        try:\n",
    "            scores[0].append(embeddings.similarity(pair[0], pair[1]))\n",
    "            scores[1].append(simlex[pair][name])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return (scores[0], scores[1])\n",
    "\n",
    "def score_simlex_pos(simlex, name, embeddings):\n",
    "    scores = [defaultdict(list), defaultdict(list)]\n",
    "    for pair in simlex:\n",
    "        try:\n",
    "            scores[0][simlex[pair][\"POS\"]].append(embeddings.similarity(pair[0], pair[1]))\n",
    "            scores[1][simlex[pair][\"POS\"]].append(simlex[pair][name])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return (scores[0], scores[1])\n",
    "\n",
    "simlex = dict()\n",
    "men = dict()\n",
    "\n",
    "with open(simlex_filename, 'r') as f:\n",
    "    headers = f.readline().split()[2:]\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        simlex[(line[0], line[1])] = dict(\n",
    "            [(header, float(score)) for header, score in zip(headers[1:], line[3:])]\n",
    "        )\n",
    "        simlex[(line[0], line[1])][headers[0]] = line[2] \n",
    "\n",
    "with open(men_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        men[(line[0], line[1])] = float(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "figure.set_size_inches(15, 5)\n",
    "for i, (data, name) in enumerate([(bow2, 'bow2'), (bow5, 'bow5'), (deps, 'deps')]):\n",
    "    yplot, xplot = score_simlex_pos(simlex, \"SimLex999\", data)\n",
    "    \n",
    "    colours = [\"blue\", \"green\", \"red\"]\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    for j, pos in enumerate(xplot.keys()):\n",
    "        plt.scatter(xplot[pos], yplot[pos], alpha=0.3, label=pos)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"SimLex999\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.title(name)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "figure.set_size_inches(15, 5)\n",
    "for i, (data, name) in enumerate([(bow2, 'bow2'), (bow5, 'bow5'), (deps, 'deps')]):\n",
    "    yplot, xplot = score_men(men, data)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(xplot, yplot, alpha=0.3)\n",
    "    plt.xlabel(\"MEN\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.title(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pearson's $\\rho$ and Spearman's $\\rho$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "results_pos = []\n",
    "for i, (data, name) in enumerate([(bow2, 'bow2'), (bow5, 'bow5'), (deps, 'deps')]):\n",
    "    # MEN\n",
    "    embed_results, gold = score_men(men, data)\n",
    "    spearman = spearmanr(embed_results, gold)\n",
    "    pearson = pearsonr(embed_results, gold)\n",
    "    results.append((name, \"MEN\", spearman[0], spearman[1], pearson[0], pearson[1]))\n",
    "\n",
    "    # SIMLEX\n",
    "    embed_results, gold = score_simlex(simlex, \"SimLex999\", data)\n",
    "    spearman = spearmanr(embed_results, gold)\n",
    "    pearson = pearsonr(embed_results, gold)\n",
    "    results.append((name, \"SimLex\", spearman[0], spearman[1], pearson[0], pearson[1]))\n",
    "    \n",
    "    # SIMLEX per POS tag\n",
    "    embed_results, gold = score_simlex_pos(simlex, \"SimLex999\", data)\n",
    "    for POS in embed_results:\n",
    "        spearman = spearmanr(embed_results[POS], gold[POS])\n",
    "        pearson = pearsonr(embed_results[POS], gold[POS])\n",
    "        results_pos.append((name, \"SimLex + {}\".format(POS), spearman[0], spearman[1], pearson[0], pearson[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation Coefficients for all pairs in the data\")\n",
    "headers = ['Embeddings', 'Gold standard', 'Spearman\\'s r',\n",
    "           'Spearman p-value', 'Pearson\\'s r', 'Pearson p-value' ]\n",
    "print(tabulate(results, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "print(\"Correlation Coefficients per POS tag\")\n",
    "headers = ['Embeddings', 'Gold standard', 'Spearman\\'s r',\n",
    "           'Spearman p-value', 'Pearson\\'s r', 'Pearson p-value' ]\n",
    "print(tabulate(results_pos, headers=headers, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analogy Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analogies = defaultdict(list)\n",
    "with open(analogy_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        if line[0] == \":\":\n",
    "            topic = line.split()[-1]\n",
    "        else:\n",
    "            analogies[topic].append(tuple(line.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(analogies, embeddings):\n",
    "    \"\"\"Calculate the accuracy and MRR for embeddings on an analogy task.\n",
    "    \n",
    "    Args:\n",
    "        analogies: dictionary with topics as keys and a list of word tuples as values\n",
    "        embeddings: dictionary of word embeddings, words as keys and vectors as values\n",
    "    Returns:\n",
    "        float: accuracy\n",
    "        float: MRR\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    all_correct = 0\n",
    "    all_analogies_in_vectors = 0\n",
    "    for topic in analogies:\n",
    "#         if topic != \"capital-common-countries\" and topic != \"capital-world\":\n",
    "#             continue\n",
    "        correct = 0\n",
    "        analogies_in_vectors = 0\n",
    "        for (a, a_star, b, b_star) in analogies[topic]:\n",
    "            \n",
    "            # If words with capital letters are not in the vectors, try to lowercase\n",
    "            if a not in embeddings and a.lower() in embeddings:\n",
    "                a = a.lower()\n",
    "            if a_star not in embeddings and a_star.lower() in embeddings:\n",
    "                a_star = a_star.lower()\n",
    "            if b not in embeddings and b.lower() in embeddings:\n",
    "                b = b.lower()\n",
    "            if b_star not in embeddings and b_star.lower() in embeddings:\n",
    "                b_star = b_star.lower()\n",
    "            if (a not in embeddings or a_star not in embeddings or\n",
    "                b not in embeddings or b_star not in embeddings):\n",
    "                continue\n",
    "\n",
    "            # Get the vector closest to the calculated analogy vector\n",
    "            analogies_in_vectors += 1\n",
    "            all_analogies_in_vectors += 1\n",
    "            b_star_embed = embeddings[b] + (embeddings[a_star] - embeddings[a])\n",
    "            b_star_embed = b_star_embed / np.linalg.norm(b_star_embed)\n",
    "            closest = embeddings.similar_by_vector(b_star_embed, topn=2)\n",
    "            b_star_approx = closest[0][0]\n",
    "            if b_star_approx == b:\n",
    "                b_star_approx = closest[1][0]\n",
    "            if b_star_approx == b_star:\n",
    "                correct += 1\n",
    "                all_correct += 1\n",
    "        results.append((topic, correct / analogies_in_vectors))\n",
    "    results.append((\"All topics\", all_correct / all_analogies_in_vectors))\n",
    "    return results\n",
    "\n",
    "        \n",
    "# sghs = KeyedVectors.load_word2vec_format(\"../honours/SGHS_d1000_i15_w25_annotated.bin\",\n",
    "#                                          binary=True)\n",
    "# sghs.init_sims(replace=True)\n",
    "results = analogy_task(analogies, bow2)\n",
    "print(tabulate(results, headers=['Topic', 'Accuracy'], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "common_words = []\n",
    "with open(common_words_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        # 'fig.' is a word, changing it to 'fig' here.\n",
    "        if line[-2] == '.':\n",
    "            line = line[:-2]\n",
    "        common_words.append(line.split()[0])\n",
    "for key in tqdm(common_words):\n",
    "    data.append(bow5[key])\n",
    "embedding = TSNE()\n",
    "result = embedding.fit_transform(data)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "plt.title('t-SNE plot of the embeddings of the 1999 common words using bow5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "whitened_data = whiten(data)\n",
    "x = []\n",
    "y = []\n",
    "for k in tqdm(range(2,40)):\n",
    "    centroids, error = kmeans(whitened_data, k)\n",
    "    x.append(k)\n",
    "    y.append(error)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "whitened_data = whiten(data)\n",
    "x = []\n",
    "y = []\n",
    "for k in tqdm(range(2,40)):\n",
    "    centroids, error = kmeans(data, k)\n",
    "    x.append(k)\n",
    "    y.append(error)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
