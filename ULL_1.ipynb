{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Language Learning, Lab1\n",
    "\n",
    "## Adriaan de Vries (10795227), Verna Dankers (10761225)\n",
    "\n",
    "Hier komt een verhaaltje over de eerste opdracht.\n",
    "\n",
    "Before being able to run this code, please import the following libraries and set the following paths to the datasets. Afterwards, the code should run without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from tabulate import tabulate\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to datasets\n",
    "bow2_filename = \"data/bow2.words\"\n",
    "bow5_filename = \"data/bow5.words\"\n",
    "deps_filename = \"data/deps.words\"\n",
    "simlex_filename = \"data/SimLex-999.txt\"\n",
    "men_filename = \"data/men/MEN_dataset_natural_form_full\"\n",
    "analogy_filename = \"data/questions-words.txt\"\n",
    "common_words_filename = \"data/common_words.words\"\n",
    "create_tables = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2word2vec(bow2_filename, bow2_filename.split(\".\")[0] + \".txt\")\n",
    "glove2word2vec(bow5_filename, bow5_filename.split(\".\")[0] + \".txt\")\n",
    "glove2word2vec(deps_filename, deps_filename.split(\".\")[0] + \".txt\")\n",
    "bow2 = KeyedVectors.load_word2vec_format(bow2_filename.split(\".\")[0] + \".txt\", binary=False)\n",
    "bow2.init_sims(replace=True)\n",
    "bow5 = KeyedVectors.load_word2vec_format(bow5_filename.split(\".\")[0] + \".txt\", binary=False)\n",
    "bow5.init_sims(replace=True)\n",
    "deps = KeyedVectors.load_word2vec_format(deps_filename.split(\".\")[0] + \".txt\", binary=False)\n",
    "deps.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collect and examine the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deps.most_similar(positive=['reddish'])[0])\n",
    "print(bow5.most_similar(positive=['cop'])[0])\n",
    "print(bow2.most_similar(positive=['tissue'])[0])\n",
    "\n",
    "print(deps.most_similar(positive=['sudoku'])[0])\n",
    "print(bow5.most_similar(positive=['sudoku'])[0])\n",
    "print(bow2.most_similar(positive=['sudoku'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Collect the SimLex and MEN data to evaluate the quality of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_men(men, embeddings):\n",
    "    scores = [[], []]\n",
    "    for pair in men:\n",
    "        try:\n",
    "            scores[0].append(embeddings.similarity(pair[0], pair[1]))\n",
    "            scores[1].append(men[pair])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return (scores[0], scores[1])\n",
    "\n",
    "def score_simlex(simlex, name, embeddings):\n",
    "    scores = [[], []]\n",
    "    for pair in simlex:\n",
    "        try:\n",
    "            scores[0].append(embeddings.similarity(pair[0], pair[1]))\n",
    "            scores[1].append(simlex[pair][name])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return (scores[0], scores[1])\n",
    "\n",
    "def score_simlex_pos(simlex, name, embeddings):\n",
    "    scores = [defaultdict(list), defaultdict(list)]\n",
    "    for pair in simlex:\n",
    "        try:\n",
    "            scores[0][simlex[pair][\"POS\"]].append(embeddings.similarity(pair[0], pair[1]))\n",
    "            scores[1][simlex[pair][\"POS\"]].append(simlex[pair][name])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return (scores[0], scores[1])\n",
    "\n",
    "simlex = dict()\n",
    "men = dict()\n",
    "\n",
    "with open(simlex_filename, 'r') as f:\n",
    "    headers = f.readline().split()[2:]\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        simlex[(line[0], line[1])] = dict(\n",
    "            [(header, float(score)) for header, score in zip(headers[1:], line[3:])]\n",
    "        )\n",
    "        simlex[(line[0], line[1])][headers[0]] = line[2] \n",
    "\n",
    "with open(men_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        men[(line[0], line[1])] = float(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "figure.set_size_inches(15, 5)\n",
    "for i, (data, name) in enumerate([(bow2, 'bow2'), (bow5, 'bow5'), (deps, 'deps')]):\n",
    "    yplot, xplot = score_simlex_pos(simlex, \"SimLex999\", data)\n",
    "    \n",
    "    colours = [\"blue\", \"green\", \"red\"]\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    for j, pos in enumerate(xplot.keys()):\n",
    "        plt.scatter(xplot[pos], yplot[pos], alpha=0.3, label=pos)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"SimLex999\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.title(name)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "figure.set_size_inches(15, 5)\n",
    "for i, (data, name) in enumerate([(bow2, 'bow2'), (bow5, 'bow5'), (deps, 'deps')]):\n",
    "    yplot, xplot = score_men(men, data)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(xplot, yplot, alpha=0.3)\n",
    "    plt.xlabel(\"MEN\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.title(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pearson's $\\rho$ and Spearman's $\\rho$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "results_pos = []\n",
    "for i, (data, name) in enumerate([(bow2, 'bow2'), (bow5, 'bow5'), (deps, 'deps')]):\n",
    "    # MEN\n",
    "    embed_results, gold = score_men(men, data)\n",
    "    spearman = spearmanr(embed_results, gold)\n",
    "    pearson = pearsonr(embed_results, gold)\n",
    "    results.append((name, \"MEN\", spearman[0], spearman[1], pearson[0], pearson[1]))\n",
    "\n",
    "    # SIMLEX\n",
    "    embed_results, gold = score_simlex(simlex, \"SimLex999\", data)\n",
    "    spearman = spearmanr(embed_results, gold)\n",
    "    pearson = pearsonr(embed_results, gold)\n",
    "    results.append((name, \"SimLex\", spearman[0], spearman[1], pearson[0], pearson[1]))\n",
    "    \n",
    "    # SIMLEX per POS tag\n",
    "    embed_results, gold = score_simlex_pos(simlex, \"SimLex999\", data)\n",
    "    for POS in embed_results:\n",
    "        spearman = spearmanr(embed_results[POS], gold[POS])\n",
    "        pearson = pearsonr(embed_results[POS], gold[POS])\n",
    "        results_pos.append((name, \"SimLex + {}\".format(POS), spearman[0], spearman[1], pearson[0], pearson[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation Coefficients for all pairs in the data\")\n",
    "headers = ['Embeddings', 'Gold standard', 'Spearman\\'s r',\n",
    "           'Spearman p-value', 'Pearson\\'s r', 'Pearson p-value' ]\n",
    "print(tabulate(results, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "print(\"Correlation Coefficients per POS tag\")\n",
    "headers = ['Embeddings', 'Gold standard', 'Spearman\\'s r',\n",
    "           'Spearman p-value', 'Pearson\\'s r', 'Pearson p-value' ]\n",
    "print(tabulate(results_pos, headers=headers, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analogy Task\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr><th style=\"text-align: right;\">     </th><th>Topic                      </th><th style=\"text-align: right;\">  Acc. Bow2</th><th style=\"text-align: right;\">  MRR Bow2</th><th style=\"text-align: right;\">  Acc. Bow5</th><th style=\"text-align: right;\">  MRR Bow5</th><th style=\"text-align: right;\">  Acc. Deps</th><th style=\"text-align: right;\">  MRR Deps</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td style=\"text-align: right;\">  992</td><td>gram1-adjective-to-adverb  </td><td style=\"text-align: right;\">  0.0413306</td><td style=\"text-align: right;\"> 0.161058 </td><td style=\"text-align: right;\">  0.0715726</td><td style=\"text-align: right;\">  0.204696</td><td style=\"text-align: right;\"> 0.0110887 </td><td style=\"text-align: right;\"> 0.0495402</td></tr>\n",
    "<tr><td style=\"text-align: right;\">  506</td><td>capital-common-countries   </td><td style=\"text-align: right;\">  0.701581 </td><td style=\"text-align: right;\"> 0.809525 </td><td style=\"text-align: right;\">  0.926877 </td><td style=\"text-align: right;\">  0.956023</td><td style=\"text-align: right;\"> 0.00592885</td><td style=\"text-align: right;\"> 0.284677 </td></tr>\n",
    "<tr><td style=\"text-align: right;\">  870</td><td>gram9-plural-verbs         </td><td style=\"text-align: right;\">  0.768966 </td><td style=\"text-align: right;\"> 0.844115 </td><td style=\"text-align: right;\">  0.703448 </td><td style=\"text-align: right;\">  0.80378 </td><td style=\"text-align: right;\"> 0.904598  </td><td style=\"text-align: right;\"> 0.942274 </td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 1599</td><td>gram6-nationality-adjective</td><td style=\"text-align: right;\">  0.57536  </td><td style=\"text-align: right;\"> 0.71328  </td><td style=\"text-align: right;\">  0.782989 </td><td style=\"text-align: right;\">  0.84296 </td><td style=\"text-align: right;\"> 0.00562852</td><td style=\"text-align: right;\"> 0.139885 </td></tr>\n",
    "<tr><td style=\"text-align: right;\">  866</td><td>currency                   </td><td style=\"text-align: right;\">  0.0123839</td><td style=\"text-align: right;\"> 0.0906957</td><td style=\"text-align: right;\">  0.0386997</td><td style=\"text-align: right;\">  0.116888</td><td style=\"text-align: right;\"> 0.0100671 </td><td style=\"text-align: right;\"> 0.0623488</td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 2467</td><td>city-in-state              </td><td style=\"text-align: right;\">  0.014998 </td><td style=\"text-align: right;\"> 0.281772 </td><td style=\"text-align: right;\">  0.369274 </td><td style=\"text-align: right;\">  0.535011</td><td style=\"text-align: right;\"> 0         </td><td style=\"text-align: right;\"> 0.139206 </td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 1560</td><td>gram7-past-tense           </td><td style=\"text-align: right;\">  0.488462 </td><td style=\"text-align: right;\"> 0.620581 </td><td style=\"text-align: right;\">  0.509615 </td><td style=\"text-align: right;\">  0.643276</td><td style=\"text-align: right;\"> 0.630128  </td><td style=\"text-align: right;\"> 0.714728 </td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 1332</td><td>gram8-plural               </td><td style=\"text-align: right;\">  0.662913 </td><td style=\"text-align: right;\"> 0.753993 </td><td style=\"text-align: right;\">  0.615616 </td><td style=\"text-align: right;\">  0.721585</td><td style=\"text-align: right;\"> 0.633634  </td><td style=\"text-align: right;\"> 0.724538 </td></tr>\n",
    "<tr><td style=\"text-align: right;\">  506</td><td>family                     </td><td style=\"text-align: right;\">  0.70751  </td><td style=\"text-align: right;\"> 0.806665 </td><td style=\"text-align: right;\">  0.756917 </td><td style=\"text-align: right;\">  0.837948</td><td style=\"text-align: right;\"> 0.786561  </td><td style=\"text-align: right;\"> 0.838142 </td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 4524</td><td>capital-world              </td><td style=\"text-align: right;\">  0.44695  </td><td style=\"text-align: right;\"> 0.615472 </td><td style=\"text-align: right;\">  0.657162 </td><td style=\"text-align: right;\">  0.771783</td><td style=\"text-align: right;\"> 0.005084  </td><td style=\"text-align: right;\"> 0.130739 </td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 1332</td><td>gram3-comparative          </td><td style=\"text-align: right;\">  0.867117 </td><td style=\"text-align: right;\"> 0.923144 </td><td style=\"text-align: right;\">  0.782282 </td><td style=\"text-align: right;\">  0.86285 </td><td style=\"text-align: right;\"> 0.707207  </td><td style=\"text-align: right;\"> 0.80205  </td></tr>\n",
    "<tr><td style=\"text-align: right;\">  812</td><td>gram2-opposite             </td><td style=\"text-align: right;\">  0.258621 </td><td style=\"text-align: right;\"> 0.365464 </td><td style=\"text-align: right;\">  0.253695 </td><td style=\"text-align: right;\">  0.366234</td><td style=\"text-align: right;\"> 0.355911  </td><td style=\"text-align: right;\"> 0.449354 </td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 1122</td><td>gram4-superlative          </td><td style=\"text-align: right;\">  0.464962 </td><td style=\"text-align: right;\"> 0.633822 </td><td style=\"text-align: right;\">  0.369318 </td><td style=\"text-align: right;\">  0.575971</td><td style=\"text-align: right;\"> 0.392992  </td><td style=\"text-align: right;\"> 0.542299 </td></tr>\n",
    "<tr><td style=\"text-align: right;\"> 1056</td><td>gram5-present-participle   </td><td style=\"text-align: right;\">  0.539773 </td><td style=\"text-align: right;\"> 0.69503  </td><td style=\"text-align: right;\">  0.628788 </td><td style=\"text-align: right;\">  0.755845</td><td style=\"text-align: right;\"> 0.619318  </td><td style=\"text-align: right;\"> 0.72413  </td></tr>\n",
    "<tr><td style=\"text-align: right;\">19544</td><td>All                        </td><td style=\"text-align: right;\">  0.440388 </td><td style=\"text-align: right;\"> 0.586389 </td><td style=\"text-align: right;\">  0.551096 </td><td style=\"text-align: right;\">  0.667913</td><td style=\"text-align: right;\"> 0.279259  </td><td style=\"text-align: right;\"> 0.389598 </td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analogies = defaultdict(list)\n",
    "with open(analogy_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        if line[0] == \":\":\n",
    "            topic = line.split()[-1]\n",
    "        else:\n",
    "            analogies[topic].append(tuple(line.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(topics, analogies, embeddings):\n",
    "    \"\"\"Calculate the accuracy and MRR for embeddings on an analogy task.\n",
    "    \n",
    "    Args:\n",
    "        analogies: dictionary with topics as keys and a list of word tuples as values\n",
    "        embeddings: dictionary of word embeddings, words as keys and vectors as values\n",
    "    Returns:\n",
    "        float: accuracy\n",
    "        float: MRR\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    mrrs = []\n",
    "    all_mrr = 0\n",
    "    all_correct = 0\n",
    "    all_present = 0\n",
    "    for topic in tqdm(topics):\n",
    "        topic_mrr = 0\n",
    "        topic_correct = 0\n",
    "        topic_present = 0\n",
    "        for (a, a_star, b, b_star) in analogies[topic]:\n",
    "            # If words with capital letters are not in the vectors, try to lowercase\n",
    "            if a not in embeddings: a = a.lower()\n",
    "            if a_star not in embeddings: a_star = a_star.lower()\n",
    "            if b not in embeddings: b = b.lower()\n",
    "            if b_star not in embeddings: b_star = b_star.lower()\n",
    "\n",
    "            # If words from the analogy are not in the embeddings, do not let it count\n",
    "            if any([word not in embeddings for word in [a, a_star, b, b_star]]):\n",
    "                continue\n",
    "\n",
    "            # Get the vector closest to the calculated analogy vector\n",
    "            b_star_approx = embeddings[b] + (embeddings[a_star] - embeddings[a])\n",
    "            b_star_approx = b_star_approx / np.linalg.norm(b_star_approx)\n",
    "            words, similarities = zip(*list(embeddings.similar_by_vector(b_star_approx, topn=5000)))\n",
    "\n",
    "            # Do not take b into account\n",
    "            words = list(words)\n",
    "            if b in words: words.remove(b)\n",
    "            b_star_approx = words[0]\n",
    "\n",
    "            # Calculate reciprocal rank for MRR\n",
    "            reciprocal_rank = 1 / (words.index(b_star) + 1) if b_star in words else 0\n",
    "            topic_mrr += reciprocal_rank\n",
    "            all_mrr += reciprocal_rank\n",
    "\n",
    "            # Calculate whether it's correct for Accuracy\n",
    "            if b_star_approx == b_star:\n",
    "                topic_correct += 1\n",
    "                all_correct += 1\n",
    "            topic_present += 1\n",
    "            all_present += 1\n",
    "\n",
    "        # Add Accuracy for this topic\n",
    "        accuracies.append(topic_correct / topic_present)\n",
    "\n",
    "        # Add MRR for this topic\n",
    "        mrrs.append(topic_mrr / topic_present)\n",
    "\n",
    "    # Calculate final Accuracy and MRR for all topics combined\n",
    "    accuracies.append(all_correct / all_present)\n",
    "    mrrs.append(all_mrr / all_present)\n",
    "    return accuracies, mrrs\n",
    "\n",
    "topics = list(analogies.keys())\n",
    "print(\"Analogy task for BOW2, this may take a while\")\n",
    "acc_bow2, mrr_bow2 = analogy_task(topics, analogies, bow2)\n",
    "\n",
    "print(\"Analogy task for BOW5, this may take a while\")\n",
    "acc_bow5, mrr_bow5 = analogy_task(topics, analogies, bow5)\n",
    "\n",
    "print(\"Analogy task for DEPS, this may take a while\")\n",
    "acc_deps, mrr_deps = analogy_task(topics, analogies, deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_tables is True:\n",
    "    topics.append('All')\n",
    "    results = [((len(analogies[line[0]]),) if line[0] != \"All\" \n",
    "               else (sum([len(analogies[t]) for t in analogies]),)) + line \n",
    "               for line in \n",
    "               zip(topics, acc_bow2, mrr_bow2, acc_bow5, mrr_bow5, acc_deps, mrr_deps) ]\n",
    "    print(tabulate(results, headers=['Topic', 'Acc. Bow2', 'MRR Bow2',\n",
    "                                     'Acc. Bow5', 'MRR Bow5',\n",
    "                                     'Acc. Deps', 'MRR Deps'], tablefmt=\"html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "common_words = []\n",
    "with open(common_words_filename, 'r') as f:\n",
    "    for line in f:\n",
    "        # 'fig.' is a word, changing it to 'fig' here.\n",
    "        if line[-2] == '.':\n",
    "            line = line[:-2]\n",
    "        common_words.append(line.split()[0])\n",
    "for key in tqdm(common_words):\n",
    "    data.append(bow5[key])\n",
    "embedding = TSNE()\n",
    "result = embedding.fit_transform(data)\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "plt.title('t-SNE plot of the embeddings of the 1999 common words using bow5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "whitened_data = whiten(data)\n",
    "x = []\n",
    "y = []\n",
    "for k in tqdm(range(2,40)):\n",
    "    centroids, error = kmeans(whitened_data, k)\n",
    "    x.append(k)\n",
    "    y.append(error)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "whitened_data = whiten(data)\n",
    "x = []\n",
    "y = []\n",
    "for k in tqdm(range(2,40)):\n",
    "    centroids, error = kmeans(data, k)\n",
    "    x.append(k)\n",
    "    y.append(error)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =(1, 2)\n",
    "b = (3,)\n",
    "b + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
